sample.frac <- 0.1
nsamp.bysource <- ceiling(table(td_df[,"source"])*sample.frac)
td_df.subsamp <-  bind_rows(td_df %>%
filter(source == "Twitter") %>%
sample_n(., nsamp.bysource["Twitter"]),
td_df %>%
filter(source == "Blogs") %>%
sample_n(., nsamp.bysource["Blogs"]),
td_df %>%
filter(source == "News") %>%
sample_n(., nsamp.bysource["News"])
)
td_df_trigrams <- td_df.subsamp %>%
unnest_tokens(ngram, text, token = "ngrams", n = 3)
### remove stop words
trigrams_separated <- td_df_trigrams %>%
separate(ngram, c("word1", "word2", "word3"), sep = " ")
trigrams_filtered <- trigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word3 %in% stop_words$word) %>%
filter(!str_detect(word1, "^\\d")) %>%
filter(!str_detect(word2, "^\\d")) %>%
filter(!str_detect(word3, "^\\d"))
trigram_counts <- trigrams_filtered %>%
group_by(source) %>%
count(word1, word2, word3, sort = TRUE) %>%
mutate(stat.cumsum =  cumsum(n/sum(n))) %>%
top_n(10, wt = n) %>%
ungroup()
trigrams_united <- trigram_counts %>%
unite(trigram, word1, word2, word3, sep = " ") %>%
arrange(source) %>%
mutate(.r = rev(row_number()),
row = row_number())
trigrams_united
trigrams_united[11:20,]
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, echo = TRUE,
message = FALSE, cache.lazy = FALSE)
#setwd("C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w1\\Coursera-SwiftKey\\test")
setwd("C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w1\\Coursera-SwiftKey\\test\\en_US")
memory.limit(size = 50000)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
## for removing stop words
data(stop_words)
twitter <- readLines("en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
twit.size <- file.info("en_US.twitter.txt")$size/1024.0^2
blogs <- readLines("en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
blogs.size <- file.info("en_US.blogs.txt")$size/1024.0^2
news <- readLines("en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
news.size <- file.info("en_US.news.txt")$size/1024.0^2
blogs.dat <- iconv(blogs, "latin1", "ASCII", sub="")
news.dat <- iconv(news, "latin1", "ASCII", sub="")
twit.dat<- iconv(twitter, "latin1", "ASCII", sub="")
rm(blogs, news, twitter)
td_df <- bind_rows(twit.dat %>%
as_tibble() %>%
mutate(source = "Twitter"),
blogs.dat %>%
as_tibble() %>%
mutate(source = "Blogs"),
news.dat %>%
as_tibble() %>%
mutate(source = "News"))
names(td_df)[1] <- "text"
uni.word.dt <- td_df %>%
unnest_tokens(word, text)
set.seed(1234)
sample.frac <- 0.1
nsamp.bysource <- ceiling(table(td_df[,"source"])*sample.frac)
td_df.subsamp <-  bind_rows(td_df %>%
filter(source == "Twitter") %>%
sample_n(., nsamp.bysource["Twitter"]),
td_df %>%
filter(source == "Blogs") %>%
sample_n(., nsamp.bysource["Blogs"]),
td_df %>%
filter(source == "News") %>%
sample_n(., nsamp.bysource["News"])
)
td_df.subsamp <- td_df.subsamp %>%
filter(!str_detect(text, "^style"))
uni.word.dt.subsamp <- td_df.subsamp %>%
unnest_tokens(word, text)
length(unique(uni.word.dt$word))
sum(uni.word.dt.subsamp$word %in% unique(uni.word.dt$word))
sum(unique(uni.word.dt.subsamp$word) %in% unique(uni.word.dt$word))
155593/596344
sum(clean.uni.word.dt$word %in% unique(uni.word.dt.subsamp$word))/dim(clean.uni.word.dt)[1]
clean.uni.word.dt <- td_df %>%
filter(!str_detect(text, "^style")) %>%
unnest_tokens(word, text)
sum(clean.uni.word.dt$word %in% unique(uni.word.dt.subsamp$word))/dim(clean.uni.word.dt)[1]
test<-c("I will have to say")
as.tibble(test)
as_tibble(test)
test<-as_tibble(test)
names(test)
names(test) <- "text"
test
test %>% unnest_tokens(word,text)
test %>% unnest_tokens(word,text) %>% ncount()
test %>% unnest_tokens(word,text) %>% count()
test2 <- test %>% unnest_tokens(word,text) %>% count()
test2
test2$n
names(test2)
test2 <- (test %>% unnest_tokens(word,text) %>% count())["n"]
test2
test <- "I see how is it"
?nchar
test <- c("I am a ", "t is r","they arler","wqrw regq r","qerw", "It was","I was")
sort(test)
setwd("C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w1\\Coursera-SwiftKey\\test\\en_US")
memory.limit(size = 50000)
## Data Exploratory
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(reshape2)
library(quanteda)
## for removing stop words
data(stop_words)
twitter <- readLines("en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
twit.size <- file.info("en_US.twitter.txt")$size/1024.0^2
blogs <- readLines("en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
blogs.size <- file.info("en_US.blogs.txt")$size/1024.0^2
news <- readLines("en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
news.size <- file.info("en_US.news.txt")$size/1024.0^2
blogs.dat <- iconv(blogs, "latin1", "ASCII", sub="")
news.dat <- iconv(news, "latin1", "ASCII", sub="")
twit.dat<- iconv(twitter, "latin1", "ASCII", sub="")
rm(blogs, news, twitter)
td_df <- bind_rows(twit.dat %>%
as_tibble() %>%
mutate(source = "Twitter"),
blogs.dat %>%
as_tibble() %>%
mutate(source = "Blogs"),
news.dat %>%
as_tibble() %>%
mutate(source = "News"))
names(td_df)[1] <- "text"
#uni.word.dt <- td_df %>%
#        unnest_tokens(word, text)
set.seed(1234)
sample.frac <- 0.001
nsamp.bysource <- ceiling(table(td_df[,"source"])*sample.frac)
td_df.subsamp <-  bind_rows(td_df %>%
filter(source == "Twitter") %>%
sample_n(., nsamp.bysource["Twitter"]),
td_df %>%
filter(source == "Blogs") %>%
sample_n(., nsamp.bysource["Blogs"]),
td_df %>%
filter(source == "News") %>%
sample_n(., nsamp.bysource["News"])
)
unigs <- td_df.subsamp %>%
unnest_tokens(ngram, text, token = "ngrams", n = 1) %>%
mutate(ngram = str_replace_all(ngram, " ", "_")) %>%
count(ngram, sort = TRUE, name = "freq") %>%
data.frame()
bigrs <- td_df.subsamp %>%
unnest_tokens(ngram, text, token = "ngrams", n = 2) %>%
mutate(ngram = str_replace_all(ngram, " ", "_")) %>%
count(ngram, sort = TRUE, name = "freq") %>%
data.frame()
trigs <- td_df.subsamp %>%
unnest_tokens(ngram, text, token = "ngrams", n = 3) %>%
mutate(ngram = str_replace_all(ngram, " ", "_")) %>%
count(ngram, sort = TRUE, name = "freq")  %>%
data.frame()
names(unigs)[1] <- "x"
names(bigrs)[1] <- "x"
names(trigs)[1] <- "x"
## Step 2. Assign probability for observed trigram
#ObservedTrig returns the observed trigrams in the corpus that starts with bigPre
getObservedTrig <- function(bigPre=bigPre, trigs=trigs){
output <- data.frame(ngrams=vector(mode='character', length=0), freq=vector(mode='integer', length=0))
trigram_index <- grep(paste0("^", bigPre,"_"), trigs$x)
output <- trigs[trigram_index,]
return(output)
}
getObservedTrigramProb <- function(ObservedTrig=ObservedTrig, bigrs=bigrs, bigPre=bigPre, gamma=gamma){
if(nrow(ObservedTrig)<1) return(NULL)
obCount <- bigrs[(bigrs$x ==bigPre),]$freq
obTrigProbs <- (ObservedTrig$freq-gamma)/obCount
output <- cbind(as.data.frame(ObservedTrig$x), obTrigProbs)
colnames(output) <- c("ngram", "prob")
row.names(output) <- 1:nrow(output)
return(output)
}
## Step 3. Assign probability for unobserved trigram
#### Step 3.1. Find words that complete unobserved trigrams
getUnobservedTrigs <- function(ObservedTrig=ObservedTrig, unigs=unigs){
observedlast <- sapply(ObservedTrig$x, FUN= function(y) paste(tail(strsplit(as.character(y), "_")[[1]],1)))
output <- unigs[!(unigs$x %in% observedlast),]$x
return(output)
}
##### Step 3.2. Calculate alpha (wi-1)
getAlphaBigr <- function(bigPre=bigPre, bigrs=bigrs, gamma=0.5){
w_i_1 <- strsplit(bigPre, "_")[[1]][2]
w_i_1 <- unigs[unigs$x==w_i_1,]
bigramcount <- bigrs[grep(paste0("^",w_i_1$x,"_"), bigrs$x),]
if (nrow(bigramcount)<1) return(1)
output <- 1- sum((bigramcount$freq-gamma)/w_i_1$freq)
return(output)
}
##### Step 3.3. Calculate backed off probabilities $$q_{BO} for bigrams
getBObigrams <- function(bigPre=bigPre, UnobservedTrigs=UnobservedTrigs){
w_i_1 <- strsplit(bigPre, "_")[[1]][2]
output <- paste0(w_i_1,"_" ,UnobservedTrigs)
return(output)
}
getObsBOBigrams <- function(bigrs=bigrs, BObigrams=BObigrams){
output <- bigrs[bigrs$x %in% BObigrams,]
return(output)
}
getUnObsBOBigrams <- function(bigrs=bigrs, BObigrams=BObigrams, ObsBOBigrams=ObsBOBigrams){
output <- BObigrams[!(BObigrams %in% ObsBOBigrams$x)]
return(output)
}
getObsBOBigramsProbs <- function(bigPre=bigPre, ObsBOBigrams=ObsBOBigrams, unigs=unigs, gamma=gamma){
w_i_1 <- strsplit(bigPre, "_")[[1]][2]
w_i_1 <- unigs[unigs$x == w_i_1,]
output <- (ObsBOBigrams$freq-gamma)/w_i_1$freq
output <- data.frame(x=ObsBOBigrams$x, prob=output)
return(output)
}
getUnObsBOBigramsProbs <- function(UnObsBOBigrams=UnObsBOBigrams, unigs=unigs, AlphaBigr=AlphaBigr){
#get the unobserved bigram tails
UnObsBOBigramsTails <- sapply(UnObsBOBigrams, FUN= function(y) paste(tail(strsplit(as.character(y), "_")[[1]],1)))
UnObsBOBigramsTails <- unigs[unigs$x %in% UnObsBOBigramsTails,]
denom <- sum(UnObsBOBigramsTails$freq)
#output <- data.frame(x=UnObsBOBigrams, prob=(AlphaBigr*UnObsBOBigramsTails$freq/denom))
output <- data.frame(x=UnObsBOBigramsTails$x, prob=(AlphaBigr*UnObsBOBigramsTails$freq/denom))
return(output)
}
### Step 4. Calculated discounted probability mass that will be distributed to unobserved trigram
### alpha (wi-2, wi-1)
getAlphaTrig <- function(bigPre=bigPre, trigs=trigs, gamma=0.5){
trigscount <- trigs[grep(paste0("^",bigPre,"_"), trigs$x),]
bigPrecount <- bigrs[bigrs$x==bigPre,]
if (nrow(trigscount)<1) return(1)
output <- 1- sum((trigscount$freq-gamma)/bigPrecount$freq)
return(output)
}
### Step 5. Calculate unobserved trigram probabilities Q_b0(wi|wi-2,wi-1)
getUnObsTrigramProbs <- function(bigPre=bigPre, QboBigrams=QboBigrams, AlphaTrig=AlphaTrig){
sumQboBigrams <- sum(QboBigrams$prob)
UnObsTrigrams <- paste(str_split(bigPre, "_")[[1]][1], QboBigrams$x, sep="_")
output <- AlphaTrig*QboBigrams$prob/sumQboBigrams
output <- data.frame(ngram=UnObsTrigrams, prob=output)
return(output)
}
## Step 6. Select wi  wi with the highest
getNextWord <- function(ObservedTrigramProb=ObservedTrigramProb, UnObsTrigramProb=UnObsTrigramProb, choices){
QboTrigrams <- rbind(ObservedTrigramProb, UnObsTrigramProb)
QboTrigrams <- QboTrigrams[order(-QboTrigrams$prob),]
QboTrigrams$ngram <- sapply(QboTrigrams$ngram, FUN= function(y) paste(tail(strsplit(as.character(y), "_")[[1]],1)))
QboTrigrams <- QboTrigrams[QboTrigrams$ngram %in% choices,]
output <- QboTrigrams[order(-QboTrigrams$prob),]
return(output)
}
predictNextWord <- function(gamma, bigPre, choices, unigs, bigrs,trigs){
ObservedTrig <- getObservedTrig(bigPre,trigs)
ObservedTrigramProb <- getObservedTrigramProb(ObservedTrig, bigrs, bigPre,gamma)
UnobservedTrigs <- getUnobservedTrigs(ObservedTrig, unigs)
AlphaBigr <- getAlphaBigr(bigPre, bigrs, gamma)
BObigrams <- getBObigrams(bigPre,UnobservedTrigs)
ObsBOBigrams <- getObsBOBigrams(bigrs, BObigrams)
UnObsBOBigrams <- getUnObsBOBigrams(bigrs=bigrs, BObigrams=BObigrams, ObsBOBigrams=ObsBOBigrams)
ObsBOBigramsProbs <- getObsBOBigramsProbs(bigPre, ObsBOBigrams, unigs, gamma)
UnObsBOBigramsProbs <- getUnObsBOBigramsProbs(UnObsBOBigrams=UnObsBOBigrams, unigs=unigs, AlphaBigr=AlphaBigr)
QboBigrams <- rbind(ObsBOBigramsProbs, UnObsBOBigramsProbs)
AlphaTrig <- getAlphaTrig(bigPre, trigs, gamma)
UnObsTrigramProbs <- getUnObsTrigramProbs(bigPre, QboBigrams, AlphaTrig)
output <- getNextWord(ObservedTrigramProb, UnObsTrigramProbs, choices)
return(output)
}
### test
gamma <- 0.5
bigPre <- "about_his"
choices <- c('spiritual', 'marital', 'horticultural', 'financial')
NextWord <- predictNextWord(gamma, bigPre, choices, unigs, bigrs,trigs)
NextWord
gamma <- 0.5
bigPre <- "here_is_president_barack"
choices <- c('spiritual', 'marital', 'horticultural', 'financial')
NextWord <- predictNextWord(gamma, bigPre, choices, unigs, bigrs,trigs)
NextWord
getNextWord.best <- function(ObservedTrigramProb=ObservedTrigramProb, UnObsTrigramProb=UnObsTrigramProb){
QboTrigrams <- rbind(ObservedTrigramProb, UnObsTrigramProb)
QboTrigrams <- QboTrigrams[order(-QboTrigrams$prob),]
QboTrigrams$ngram <- sapply(QboTrigrams$ngram, FUN= function(y) paste(tail(strsplit(as.character(y), "_")[[1]],1)))
#QboTrigrams <- QboTrigrams[QboTrigrams$ngram %in% choices,]
QboTrigrams <- QboTrigrams
output <- QboTrigrams[order(-QboTrigrams$prob),][1]
return(output)
}
gamma <- 0.5
bigPre <- "here_is_president_barack"
#choices <- c('spiritual', 'marital', 'horticultural', 'financial')
NextWord <- predictNextWord.best(gamma, bigPre,  unigs, bigrs,trigs)
NextWord
predictNextWord.best <- function(gamma, bigPre, unigs, bigrs,trigs){
ObservedTrig <- getObservedTrig(bigPre,trigs)
ObservedTrigramProb <- getObservedTrigramProb(ObservedTrig, bigrs, bigPre,gamma)
UnobservedTrigs <- getUnobservedTrigs(ObservedTrig, unigs)
AlphaBigr <- getAlphaBigr(bigPre, bigrs, gamma)
BObigrams <- getBObigrams(bigPre,UnobservedTrigs)
ObsBOBigrams <- getObsBOBigrams(bigrs, BObigrams)
UnObsBOBigrams <- getUnObsBOBigrams(bigrs=bigrs, BObigrams=BObigrams, ObsBOBigrams=ObsBOBigrams)
ObsBOBigramsProbs <- getObsBOBigramsProbs(bigPre, ObsBOBigrams, unigs, gamma)
UnObsBOBigramsProbs <- getUnObsBOBigramsProbs(UnObsBOBigrams=UnObsBOBigrams, unigs=unigs, AlphaBigr=AlphaBigr)
QboBigrams <- rbind(ObsBOBigramsProbs, UnObsBOBigramsProbs)
AlphaTrig <- getAlphaTrig(bigPre, trigs, gamma)
UnObsTrigramProbs <- getUnObsTrigramProbs(bigPre, QboBigrams, AlphaTrig)
output <- getNextWord.best(ObservedTrigramProb, UnObsTrigramProbs)
return(output)
}
gamma <- 0.5
bigPre <- "here_is_president_barack"
#choices <- c('spiritual', 'marital', 'horticultural', 'financial')
NextWord <- predictNextWord.best(gamma, bigPre,  unigs, bigrs,trigs)
NextWord
getNextWord.best <- function(ObservedTrigramProb=ObservedTrigramProb, UnObsTrigramProb=UnObsTrigramProb){
QboTrigrams <- rbind(ObservedTrigramProb, UnObsTrigramProb)
QboTrigrams <- QboTrigrams[order(-QboTrigrams$prob),]
QboTrigrams$ngram <- sapply(QboTrigrams$ngram, FUN= function(y) paste(tail(strsplit(as.character(y), "_")[[1]],1)))
#QboTrigrams <- QboTrigrams[QboTrigrams$ngram %in% choices,]
QboTrigrams <- QboTrigrams
output <- QboTrigrams[order(-QboTrigrams$prob),]
return(output[1,])
}
predictNextWord.best <- function(gamma, bigPre, unigs, bigrs,trigs){
ObservedTrig <- getObservedTrig(bigPre,trigs)
ObservedTrigramProb <- getObservedTrigramProb(ObservedTrig, bigrs, bigPre,gamma)
UnobservedTrigs <- getUnobservedTrigs(ObservedTrig, unigs)
AlphaBigr <- getAlphaBigr(bigPre, bigrs, gamma)
BObigrams <- getBObigrams(bigPre,UnobservedTrigs)
ObsBOBigrams <- getObsBOBigrams(bigrs, BObigrams)
UnObsBOBigrams <- getUnObsBOBigrams(bigrs=bigrs, BObigrams=BObigrams, ObsBOBigrams=ObsBOBigrams)
ObsBOBigramsProbs <- getObsBOBigramsProbs(bigPre, ObsBOBigrams, unigs, gamma)
UnObsBOBigramsProbs <- getUnObsBOBigramsProbs(UnObsBOBigrams=UnObsBOBigrams, unigs=unigs, AlphaBigr=AlphaBigr)
QboBigrams <- rbind(ObsBOBigramsProbs, UnObsBOBigramsProbs)
AlphaTrig <- getAlphaTrig(bigPre, trigs, gamma)
UnObsTrigramProbs <- getUnObsTrigramProbs(bigPre, QboBigrams, AlphaTrig)
output <- getNextWord.best(ObservedTrigramProb, UnObsTrigramProbs)
return(output)
}
gamma <- 0.5
bigPre <- "here_is_president_barack"
#choices <- c('spiritual', 'marital', 'horticultural', 'financial')
NextWord <- predictNextWord.best(gamma, bigPre,  unigs, bigrs,trigs)
NextWord
gamma <- 0.5
bigPre <- "how_are"
#choices <- c('spiritual', 'marital', 'horticultural', 'financial')
NextWord <- predictNextWord.best(gamma, bigPre,  unigs, bigrs,trigs)
NextWord
gamma <- 0.5
bigPre <- "what_are_you"
#choices <- c('spiritual', 'marital', 'horticultural', 'financial')
NextWord <- predictNextWord.best(gamma, bigPre,  unigs, bigrs,trigs)
NextWord
input.text <- "We could see"
nchar(input.text)
length(input.text)
tibble(input.text)
test<-tibble(input.text)
test
test<-unnest_tokens(tibble(input.text))
test<-unnest_tokens(tibble(input.text),word,input.text)
test
save(bigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\bigrams_dt.Data")
save(trigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\trigrams_dt.Data")
uni.word.dt.subsamp <- td_df.subsamp %>%
filter(!str_detect(text, "^style")) %>%
unnest_tokens(word, text) %>%
filter(!str_detect(word, "^\\d")) %>%
count(word, sort = TRUE) %>%
mutate(prob =  cumsum(n/sum(n)))
td_df_bigrams <- td_df.subsamp %>%
unnest_tokens(ngram, text, token = "ngrams", n = 2)
### remove stop words
bigrams_separated <- td_df_bigrams %>%
separate(ngram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!str_detect(word1, "^\\d")) %>%
filter(!str_detect(word2, "^\\d"))
bigrams_dt_prob <- bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
mutate(prob =  cumsum(n/sum(n))) %>%
arrange(desc(n))
td_df_trigrams <- td_df.subsamp %>%
unnest_tokens(ngram, text, token = "ngrams", n = 3)
### remove stop words
trigrams_separated <- td_df_trigrams %>%
separate(ngram, c("word1", "word2", "word3"), sep = " ")
trigrams_filtered <- trigrams_separated %>%
filter(!str_detect(word1, "^\\d")) %>%
filter(!str_detect(word2, "^\\d")) %>%
filter(!str_detect(word3, "^\\d"))
trigrams_dt_prob <- trigrams_filtered %>%
count(word1, word2,word3, sort = TRUE) %>%
mutate(prob =  cumsum(n/sum(n))) %>%
arrange(desc(n))
save(bigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\bigrams_dt.Data")
save(trigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\trigrams_dt.Data")
input.text <- "president barack obama said"
get.next.text <- function(input.text,trigrams_dt_prob,bigrams_dt_prob){
best.text <- "unfound"
tb_input.text <- as_tibble(input.text)
names(tb_input.text) <- "text"
uni.word.input <- tb_input.text %>%
unnest_tokens(word, text)
n.word.count <- dim(uni.word.input)[1]
last.two.word.input <- uni.word.input$word[c((n.word.count-1),n.word.count)]
match.last.twoword.input <-subset(trigrams_dt_prob,trigrams_dt_prob$word2==last.two.word.input[2] &
trigrams_dt_prob$word1==last.two.word.input[1] )
### calculate observed word
if(length(match.last.twoword.input$word1) >=1){
obs.trigram.prob <- match.last.twoword.input %>%
count(word1, word2, word3, sort = TRUE) %>%
mutate(prob =  (n/sum(n))) %>%
arrange(desc(n)) %>%
top_n(1)
best.text <- obs.trigram.prob$word3[1]
} ### end if any match
### calculate unobserved word
if(length(match.last.twoword.input$word1) ==0){
match.bigram.last.word.input <- subset(bigrams_dt_prob,bigrams_dt_prob$word1==last.two.word.input[2])
if(length(match.bigram.last.word.input$word1) >=1){
obs.bigram.prob <- match.bigram.last.word.input %>%
count(word1, word2, sort = TRUE) %>%
mutate(prob =  cumsum(n/sum(n))) %>%
arrange(desc(n)) %>%
top_n(1)
best.text <- obs.bigram.prob$word2[1]
} ### end if matching bigram
} ### end if no match
return(best.text)
}
input.text <- "In 1999, president Barack"
get.next.text(input.text,trigrams_dt_prob,bigrams_dt_prob)
input.text <- "I'm cleaning the"
get.next.text(input.text,trigrams_dt_prob,bigrams_dt_prob)
input.text <- "how should I"
get.next.text(input.text,trigrams_dt_prob,bigrams_dt_prob)
input.text <- "how are"
get.next.text(input.text,trigrams_dt_prob,bigrams_dt_prob)
shiny::runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
save(bigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\bigrams_dt.RData")
save(trigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\trigrams_dt.RData")
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
save(bigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\bigrams_dt.rds")
save(trigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\trigrams_dt.rds")
saveRDS(bigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\bigrams_dt.rds")
saveRDS(trigrams_dt_prob, file="C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\shiny\\Prednextword\\trigrams_dt.rds")
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
runApp('C:/Users/htsai4/Desktop/temp/course/data science/Cap/w5/shiny/Prednextword')
setwd("C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\slidify")
library(slidify)
author('test')
library(knitr)
slidify("index.html")
slidify("\\test\\index.html")
browseURL("\\test\\index.html")
slidify("\\test\\index.html")
slidify("\\test\\index.html")
slidify("\\test\\index.Rmd")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
setwd("C:\\Users\\htsai4\\Desktop\\temp\\course\\data science\\Cap\\w5\\slidify\\test2")
slidify("index.Rmd")
browseURL("index.html")
slidify("index.Rmd")
browseURL("index.html")
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
slidify("index.Rmd",
options=list(framework = 'deck.js',theme = 'swiss',
transition = 'horizontal-slide'))
library(devtools)
install_github('ramnathv/slidify')
install_github('ramnathv/slidifyLibraries')
library(slidify)
